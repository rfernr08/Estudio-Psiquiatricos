0-8-16-24{"learning_rate": 2e-5, "batch_size": 16, "epochs": 5, "weight_decay": 0.01, "warmup_ratio": 0.1},
1-9-17-25{"learning_rate": 2e-5, "batch_size": 32, "epochs": 5, "weight_decay": 0.01, "warmup_ratio": 0.1},
2-10-18-26{"learning_rate": 3e-5, "batch_size": 16, "epochs": 4, "weight_decay": 0.001, "warmup_ratio": 0.0},
3-11-19-27{"learning_rate": 3e-5, "batch_size": 32, "epochs": 4, "weight_decay": 0.001, "warmup_ratio": 0.1},
4-12-20-28{"learning_rate": 3e-5, "batch_size": 64, "epochs": 4, "weight_decay": 0.001, "warmup_ratio": 0.1},
5-13-21-29{"learning_rate": 5e-5, "batch_size": 32, "epochs": 3, "weight_decay": 0.0, "warmup_ratio": 0.0},
6-14-22-30{"learning_rate": 1e-5, "batch_size": 32, "epochs": 5, "weight_decay": 0.01, "warmup_ratio": 0.1},
7-15-23-31{"learning_rate": 1e-5, "batch_size": 64, "epochs": 5, "weight_decay": 0.01, "warmup_ratio": 0.1},

üèÜ Mejores resultados por F1:
                         modelo  learning_rate  batch_size  epochs  accuracy  \
5  bert-base-multilingual-cased        0.00005          32       3  0.740260   
4  bert-base-multilingual-cased        0.00003          64       4  0.716450   
0  bert-base-multilingual-cased        0.00002          16       5  0.759740   
3  bert-base-multilingual-cased        0.00003          32       4  0.757576   
6  bert-base-multilingual-cased        0.00001          32       5  0.681818   
7  bert-base-multilingual-cased        0.00001          64       5  0.699134   
2  bert-base-multilingual-cased        0.00003          16       4  0.651515   
1  bert-base-multilingual-cased        0.00002          32       5  0.716450   

         f1  precision    recall  
5  0.612903   0.590062  0.637584  -
4  0.599388   0.550562  0.657718  
0  0.564706   0.679245  0.483221  
3  0.548387   0.686869  0.456376  
6  0.514851   0.506494  0.523490  
7  0.501792   0.538462  0.469799  
2  0.465116   0.460526  0.469799  
1  0.451883   0.600000  0.362416  

üìä Mejores modelos por score combinado (f1 + recall):
                         modelo  learning_rate  batch_size  epochs  accuracy  \
5  bert-base-multilingual-cased        0.00005          32       3  0.740260   
4  bert-base-multilingual-cased        0.00003          64       4  0.716450   
0  bert-base-multilingual-cased        0.00002          16       5  0.759740   
6  bert-base-multilingual-cased        0.00001          32       5  0.681818   
3  bert-base-multilingual-cased        0.00003          32       4  0.757576   
7  bert-base-multilingual-cased        0.00001          64       5  0.699134   
2  bert-base-multilingual-cased        0.00003          16       4  0.651515   
1  bert-base-multilingual-cased        0.00002          32       5  0.716450   

         f1  precision    recall     score  
5  0.612903   0.590062  0.637584  0.622775  
4  0.599388   0.550562  0.657718  0.622720  
0  0.564706   0.679245  0.483221  0.532112  
6  0.514851   0.506494  0.523490  0.518307  
3  0.548387   0.686869  0.456376  0.511583  
7  0.501792   0.538462  0.469799  0.488995  
2  0.465116   0.460526  0.469799  0.466989  
1  0.451883   0.600000  0.362416  0.416096  

üèÜ Mejores resultados por F1:
                                  modelo  learning_rate  batch_size  epochs  \
5  dccuchile/bert-base-spanish-wwm-cased        0.00005          32       3   
0  dccuchile/bert-base-spanish-wwm-cased        0.00002          16       5   
2  dccuchile/bert-base-spanish-wwm-cased        0.00003          16       4   
1  dccuchile/bert-base-spanish-wwm-cased        0.00002          32       5   
3  dccuchile/bert-base-spanish-wwm-cased        0.00003          32       4   
7  dccuchile/bert-base-spanish-wwm-cased        0.00001          64       5   
6  dccuchile/bert-base-spanish-wwm-cased        0.00001          32       5   
4  dccuchile/bert-base-spanish-wwm-cased        0.00003          64       4   

   accuracy        f1  precision    recall  
5  0.759740  0.602151   0.646154  0.563758  
0  0.770563  0.595420   0.690265  0.523490  
2  0.764069  0.594796   0.666667  0.536913  
1  0.768398  0.593156   0.684211  0.523490  
3  0.753247  0.547619   0.669903  0.463087  
7  0.746753  0.544747   0.648148  0.469799  
6  0.740260  0.538462   0.630631  0.469799  
4  0.738095  0.480687   0.666667  0.375839  

üìä Mejores modelos por score combinado (f1 + recall):
                                  modelo  learning_rate  batch_size  epochs  \
5  dccuchile/bert-base-spanish-wwm-cased        0.00005          32       3   
2  dccuchile/bert-base-spanish-wwm-cased        0.00003          16       4   
0  dccuchile/bert-base-spanish-wwm-cased        0.00002          16       5   
1  dccuchile/bert-base-spanish-wwm-cased        0.00002          32       5   
7  dccuchile/bert-base-spanish-wwm-cased        0.00001          64       5   
3  dccuchile/bert-base-spanish-wwm-cased        0.00003          32       4   
6  dccuchile/bert-base-spanish-wwm-cased        0.00001          32       5   
4  dccuchile/bert-base-spanish-wwm-cased        0.00003          64       4   

   accuracy        f1  precision    recall     score  
5  0.759740  0.602151   0.646154  0.563758  0.586794  
2  0.764069  0.594796   0.666667  0.536913  0.571642  
0  0.770563  0.595420   0.690265  0.523490  0.566648  
1  0.768398  0.593156   0.684211  0.523490  0.565290  
7  0.746753  0.544747   0.648148  0.469799  0.514768  
3  0.753247  0.547619   0.669903  0.463087  0.513806  
6  0.740260  0.538462   0.630631  0.469799  0.510996  
4  0.738095  0.480687   0.666667  0.375839  0.438748  